{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "198bacae-560a-47a8-b4d3-9f35acaacb41",
   "metadata": {},
   "source": [
    " <font size=6> <b>Working with WARC Files</b></font> <br>\n",
    " <font size=4> The Gold Standard to Create Own Collections of Webdata in Panelformat</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1151d6-dd76-41b2-adb3-2fb53803eada",
   "metadata": {},
   "source": [
    "by *Julian Oliver Dörr*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f5d45e-c6c1-4365-8d7a-f8e3e1367e77",
   "metadata": {},
   "source": [
    "This notebook shows how to work with files in the **Web ARChive (WARC)** format. The **WARC** format represents the unique **ISO standard** ([ISO 28500:2017](https://www.iso.org/standard/68004.html)) format for archiving webdata. With the increased crawling and storing of World Wide Web material in the late 90s, the need for a standardized format to store such content started to become a serious issue. It is for this reason that the WARC format (previously ARC format) was developed in order to allow standardized storage, management and exchange of digital objects from the web. Today WARC is recognised by most national library systems as the standard to follow for web archiving and all major web archives such as [Common Crawl](https://commoncrawl.org/the-data/get-started/) (CC) and the [Internet Archive](https://archive.org/) (IA) use the WARC format as gold standard to store their web crawls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d8d99-104f-4e08-abe1-9ee14705b7a2",
   "metadata": {},
   "source": [
    "In simple terms, a WARC file consists of two parts:\n",
    "1. **header** information including compulsory metadata such as a Uniform Resource Identifier (URI) which is typically the website's URL, crawling date, content type and length of the record as well as non-compulsory metadata\n",
    "2. **cotent** block which comprises the actual content (the so called payload) found on the webpage (e.g. html code, pdfs, images, videos)\n",
    "\n",
    "The International Internet Preservation Consortium ([IIPC](https://netpreserve.org/)) provides a detailed description of the WARC file format and its different elements which can be found [here](https://iipc.github.io/warc-specifications/specifications/warc-format/warc-1.0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f4ddea-bf9b-4ba9-a298-f53d4630d749",
   "metadata": {},
   "source": [
    "The following code snippets show how to create own WARC files from both crawling live websites and from generating collections from existing web archives (such as CC and IA). For this purpose, we work with the Python module [`warcio`](https://github.com/webrecorder/warcio). warcio provides a standalone way to read and write WARC files compliant with the WARC ISO standard. Important to mention, the module is designed for fast, low-level access to web archival content, oriented around a stream of WARC records rather than files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fce20d65-a971-42ec-8bda-66b343861cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warcio.warcwriter import WARCWriter\n",
    "from warcio.statusandheaders import StatusAndHeaders\n",
    "import requests # required to access live website content given URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17683298-5c4a-406f-a472-54299373e0ce",
   "metadata": {},
   "source": [
    "Here we define two functions that allows us the return the size of files (required later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1728db13-468a-4bc7-a1c9-d4fc9b856e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "\n",
    "def get_uncompressed_size(FileName):\n",
    "    with gzip.open(FileName, 'rb') as fd:\n",
    "        fd.seek(0, 2)\n",
    "        size = fd.tell()\n",
    "    return print(size/1000000, 'MB')\n",
    "\n",
    "def get_compressed_size(filename):\n",
    "    return print(str(os.stat(filename).st_size/1000000), 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34b7629-3dde-4c77-a440-3a26114d511b",
   "metadata": {},
   "source": [
    "# WARC files form live websites "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fe28b3-9c70-4da8-95d5-eda212109c5a",
   "metadata": {},
   "source": [
    "First, we conduct the crawling by accessing the starting pages for a set of corporate websites and the store the response elements as WARC files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a5328-e4a3-43b0-bcf9-e7355212a03f",
   "metadata": {},
   "source": [
    "## Write .warc files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99873ba-4581-4208-8c95-7d7d3fea9afb",
   "metadata": {},
   "source": [
    "First we define a short list of (corporate) URLs whose current content we want to store in a WARC file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d6ffab-9bab-4595-9c33-ce0753bd78c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://www.zew.de/', 'https://istari.ai/', 'https://new.siemens.com/']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4cc6f4-6f88-4626-bbee-37ddb8f68ea4",
   "metadata": {},
   "source": [
    "We now create a .warc file and stream the web contents found on the above websites into the file adhering to the WARC 1.0 ISO standard. Note that we compress the .warc via the GZIP format which allows significant storage savings. GZIP is widely used and supported across many free and commercial software packages and operating systems. Compressing via GZIP is commonly done when creating .warc files and the reason why most WARC files end with .warc.gz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9126e499-d120-4cfe-a3e1-60c499e75d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Q:\\Meine Bibliotheken\\Research\\05_Code\\02_ArchiveSpark\\example.warc.gz', 'wb') as output:\n",
    "    writer = WARCWriter(output, gzip=True) # instantiate the warc writer object which allows to write the retrieved webdate into the example.warc.gz file\n",
    "\n",
    "    for url in urls:\n",
    "        # get the webserver's response to the HTTP request ...\n",
    "        response = requests.get(\n",
    "            url,                            # ... of the given URL\n",
    "            headers={'Accept-Encoding': 'identity'},\n",
    "            stream=True                     # by default, when you make a request, the body of the response is downloaded immediately.\n",
    "                               )            # Override this behaviour and defer downloading the response body until you access \n",
    "                                            # the Response.content attribute by setting stream = True\n",
    "                                           \n",
    "        # get raw headers from the response\n",
    "        headers_list = response.raw.headers.items()\n",
    "        # append url to headers_list\n",
    "        headers_list.append(('URL', url))\n",
    "        \n",
    "        # from the raw headers create a WARC compatible header\n",
    "        http_headers = StatusAndHeaders('200 OK', headers_list, protocol='HTTP/1.0')\n",
    "        \n",
    "        # create WARC record ...\n",
    "        record = writer.create_warc_record(\n",
    "            uri=url,                         # ... of the given URL which serves as Uniform Resource Identifier (URI) of the record\n",
    "            record_type='response',\n",
    "            payload=response.raw,            # content block found on website\n",
    "            http_headers=http_headers)       # header information (metadata) of respective website\n",
    "\n",
    "        writer.write_record(record)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b32ce4-cbe0-4e92-b333-28c330a08e3c",
   "metadata": {},
   "source": [
    "The above code snippet has created gzipped WARC file. The size of both the compressed and the uncompressed file is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "052c4456-d6b0-45e2-89e7-fd4d5a1543f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.291773 MB\n",
      "0.053284 MB\n"
     ]
    }
   ],
   "source": [
    "get_uncompressed_size(r'Q:\\Meine Bibliotheken\\Research\\05_Code\\02_ArchiveSpark\\example.warc.gz')\n",
    "get_compressed_size(r'Q:\\Meine Bibliotheken\\Research\\05_Code\\02_ArchiveSpark\\example.warc.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48842fab-3f74-4398-a20f-2dbbd4539282",
   "metadata": {},
   "source": [
    "We see that the file size reduces by factor 5 when using GZIP for compressing the WARC file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-bandwidth",
   "metadata": {},
   "source": [
    "## Read .warc files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-gibson",
   "metadata": {},
   "source": [
    "The stored WARC file and its headers and payloads can be easily accessed via warcio's `ArchiveIterator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "54fad7ea-847d-4bf6-8959-e423b07eb458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warcio.archiveiterator import ArchiveIterator\n",
    "import pandas as pd # used to store results in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4ab1126e-e7d8-4893-ad58-67e391ce5505",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_header = pd.DataFrame()\n",
    "with open(r'Q:\\Meine Bibliotheken\\Research\\05_Code\\02_ArchiveSpark\\example.warc.gz', 'rb') as stream:\n",
    "    for record in ArchiveIterator(stream):\n",
    "        temp = sorted([(i[0], [i[1]]) for i in record.http_headers.headers if i[0] in ['Date', 'Content-Type', 'Content-Length', 'URL']], key=lambda tup: tup[0], reverse=True) \n",
    "        df_temp = pd.DataFrame.from_dict(dict(temp))\n",
    "        df_header = df_header.append(df_temp)\n",
    "stream.close()\n",
    "\n",
    "df_header.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cb046d21-bee1-403d-88ac-7d6aaecb9972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Content-Type</th>\n",
       "      <th>Content-Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.zew.de/</td>\n",
       "      <td>Tue, 27 Jul 2021 12:00:27 GMT</td>\n",
       "      <td>text/html; charset=utf-8</td>\n",
       "      <td>75275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://istari.ai/</td>\n",
       "      <td>Tue, 27 Jul 2021 12:00:27 GMT</td>\n",
       "      <td>text/html</td>\n",
       "      <td>194894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://new.siemens.com/</td>\n",
       "      <td>Tue, 27 Jul 2021 12:00:29 GMT</td>\n",
       "      <td>text/html; charset=utf-8</td>\n",
       "      <td>16954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        URL                           Date  \\\n",
       "0       https://www.zew.de/  Tue, 27 Jul 2021 12:00:27 GMT   \n",
       "1        https://istari.ai/  Tue, 27 Jul 2021 12:00:27 GMT   \n",
       "2  https://new.siemens.com/  Tue, 27 Jul 2021 12:00:29 GMT   \n",
       "\n",
       "               Content-Type Content-Length  \n",
       "0  text/html; charset=utf-8          75275  \n",
       "1                 text/html         194894  \n",
       "2  text/html; charset=utf-8          16954  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed653e4f-a018-42f1-8e64-96844169187f",
   "metadata": {},
   "source": [
    "We can see that WARC file stores the following metadata: `URL` of the scraped website, `Date` of scraping, the `Content-Type` found on the website which is in all instances html text and finally the `Content-Length` of the content block (in bytes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d24c42-f866-4773-a4f0-0049cd41f588",
   "metadata": {},
   "source": [
    "Besides, the header information the WARC files obviously also include the content blocks which we access now using warcio's `content_stream`. We further use `BeautifulSoup` to parse the stored html content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "secret-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8570affe-c608-413b-bd9b-d317c5a1fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content = pd.DataFrame()\n",
    "with open(r'Q:\\Meine Bibliotheken\\Research\\05_Code\\02_ArchiveSpark\\example.warc.gz', 'rb') as stream:\n",
    "    for record in ArchiveIterator(stream):\n",
    "        temp = [record.rec_headers.get_header('WARC-Target-URI'), BeautifulSoup(record.content_stream().read(), 'html.parser').get_text().replace('\\n', \" \").replace('\\t', \" \").strip()]\n",
    "        df_temp = pd.DataFrame([temp], columns=['URL', 'html/text'])\n",
    "        df_content = df_content.append(df_temp)\n",
    "stream.close()\n",
    "\n",
    "df_content.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "deac72d7-7bf4-471d-b517-c982105c838b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>html/text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.zew.de/</td>\n",
       "      <td>ZEW – Leibniz-Zentrum für Europäische Wirtschaftsforschung - Startseite                                                         Menü              Presse   Team   Karriere   ZEW-Newsletter   Kontakt   ZEWnews                                                               DE                                                                             EN                                             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://istari.ai/</td>\n",
       "      <td>ISTARI.AI - Die Zukunft der Unternehmensdatenbank                                                                                                                              Primary Menu HOME TEAM REFERENZEN WEBAI NEWS KONTAKT                        Die Zukunft der Unternehmensdatenbank – Umfangreiche Informationen in Echtzeit       Verborgene Unternehmensdaten automatisiert sichtbar machen, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://new.siemens.com/</td>\n",
       "      <td>SiemensWe're sorry but the new Siemens doesn't work properly without JavaScript enabled. Please enable it to continue.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        URL  \\\n",
       "0       https://www.zew.de/   \n",
       "1        https://istari.ai/   \n",
       "2  https://new.siemens.com/   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                         html/text  \n",
       "0  ZEW – Leibniz-Zentrum für Europäische Wirtschaftsforschung - Startseite                                                         Menü              Presse   Team   Karriere   ZEW-Newsletter   Kontakt   ZEWnews                                                               DE                                                                             EN                                             ...  \n",
       "1  ISTARI.AI - Die Zukunft der Unternehmensdatenbank                                                                                                                              Primary Menu HOME TEAM REFERENZEN WEBAI NEWS KONTAKT                        Die Zukunft der Unternehmensdatenbank – Umfangreiche Informationen in Echtzeit       Verborgene Unternehmensdaten automatisiert sichtbar machen, ...  \n",
       "2                                                                                                                                                                                                                                                                                           SiemensWe're sorry but the new Siemens doesn't work properly without JavaScript enabled. Please enable it to continue.  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 400)\n",
    "df_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471bd547-2575-4fa9-9bfb-0b5ef96e640c",
   "metadata": {},
   "source": [
    "# WARC files from existing web archives "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dac09bc-bde4-4a31-b1c6-3d52c58f8570",
   "metadata": {},
   "source": [
    "In a second step, we create our own collection of websites that have already been stored in web archives by saving these historcial web contents into WARC files. This means that we retrieve only the relevant websites stored in existing web archives such as CC and IA. In our context this includes historical website content of corporate web domains only! Note that CC's and IA's web archives comprise way more than just corporate websites. That is why it is impractical to work with their WARC files but rather it is neccessary to create domain specific collections of own WARC files. In the following, we will work with `cdx_toolkit` which allows to access CC's and IA's CDX API for structured access to the archives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "respiratory-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdx_toolkit\n",
    "from tqdm import tqdm # required for showing progress in accessing web archives\n",
    "import pandas as pd\n",
    "import re             # required for wildcarding URLs (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b1c5d-683c-4adf-9780-15857e81d286",
   "metadata": {},
   "source": [
    "## Access web archives and write .warc files  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb4b64-decf-49e2-8328-f588e83ccdeb",
   "metadata": {},
   "source": [
    "First, we load a number of year-url combinations for a sample of corporate web domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "fifth-dollar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crefo</th>\n",
       "      <th>year</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5050292256</td>\n",
       "      <td>2010</td>\n",
       "      <td>www.tulip.de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5050292256</td>\n",
       "      <td>2011</td>\n",
       "      <td>www.tulip.de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5050292256</td>\n",
       "      <td>2012</td>\n",
       "      <td>www.tulip.de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5050292256</td>\n",
       "      <td>2013</td>\n",
       "      <td>www.tulip.de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5050292256</td>\n",
       "      <td>2014</td>\n",
       "      <td>www.tulip.de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>7290613987</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>7290613987</td>\n",
       "      <td>2017</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>7290613987</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>7290613987</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>7290613987</td>\n",
       "      <td>2020</td>\n",
       "      <td>www.martinshof.de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1947 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           crefo  year                url\n",
       "0     5050292256  2010       www.tulip.de\n",
       "1     5050292256  2011       www.tulip.de\n",
       "2     5050292256  2012       www.tulip.de\n",
       "3     5050292256  2013       www.tulip.de\n",
       "4     5050292256  2014       www.tulip.de\n",
       "...          ...   ...                ...\n",
       "1942  7290613987  2016                NaN\n",
       "1943  7290613987  2017                NaN\n",
       "1944  7290613987  2018                NaN\n",
       "1945  7290613987  2019                NaN\n",
       "1946  7290613987  2020  www.martinshof.de\n",
       "\n",
       "[1947 rows x 3 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_url = pd.read_csv(r\"Q:\\Meine Bibliotheken\\Research\\01_Promotion\\05_Ideas\\06_GreenFinance\\05_Data\\mup2afid_urls_sample.txt\", sep = \"\\t\")\n",
    "df_url.loc[(df_url.crefo==3270030744) & (df_url.year.isin([2019, 2020])),'url'] = \"www.thomas-gruppe.de\" # minor manual correction\n",
    "df_mup = pd.read_csv(r\"Q:\\Meine Bibliotheken\\Research\\01_Promotion\\05_Ideas\\06_GreenFinance\\05_Data\\mup2afid.txt\", sep=\"\\t\")\n",
    "df_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-faith",
   "metadata": {},
   "source": [
    "When accessing web archives, it is most convenient to wildcard the corporate web domain's start page as this way all subpages will be extracted from the archives as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "freelance-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that allows widlcarding of URLs\n",
    "def wildcarding(url):\n",
    "    url_wildcarded = re.search(r'\\..{1,}\\.(?:com|de|rwe|net|eu|fr|heise-service)', url).group(0)[1:] + '/*'\n",
    "    return url_wildcarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "humanitarian-america",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Reset index for loop\n",
    "df_url = df_url.reset_index(drop=True)\n",
    "# Check if any url cannot be wildcarded\n",
    "ind = []\n",
    "for i in range(df_url.shape[0]):\n",
    "    if pd.notna(df_url.url[i]):\n",
    "        try:\n",
    "            wildcarding(df_url.url[i])\n",
    "        except:\n",
    "            ind.append(i)\n",
    "            \n",
    "# If resulting list empty all URLs have been wildcarded sucessfully, else the list returns the index of URLs in df_url where wildcarding failed\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "adverse-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct the wildcarding\n",
    "df_url.loc[df_url.url.notnull(), 'url'] = df_url.loc[df_url.url.notnull(), 'url'].apply(lambda x: wildcarding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "humanitarian-sunrise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crefo</th>\n",
       "      <th>year</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5050292256</td>\n",
       "      <td>2010</td>\n",
       "      <td>tulip.de/*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5050292256</td>\n",
       "      <td>2011</td>\n",
       "      <td>tulip.de/*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5050292256</td>\n",
       "      <td>2012</td>\n",
       "      <td>tulip.de/*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5050292256</td>\n",
       "      <td>2013</td>\n",
       "      <td>tulip.de/*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5050292256</td>\n",
       "      <td>2014</td>\n",
       "      <td>tulip.de/*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>7290613987</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>7290613987</td>\n",
       "      <td>2017</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>7290613987</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>7290613987</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>7290613987</td>\n",
       "      <td>2020</td>\n",
       "      <td>martinshof.de/*</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1947 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           crefo  year              url\n",
       "0     5050292256  2010       tulip.de/*\n",
       "1     5050292256  2011       tulip.de/*\n",
       "2     5050292256  2012       tulip.de/*\n",
       "3     5050292256  2013       tulip.de/*\n",
       "4     5050292256  2014       tulip.de/*\n",
       "...          ...   ...              ...\n",
       "1942  7290613987  2016              NaN\n",
       "1943  7290613987  2017              NaN\n",
       "1944  7290613987  2018              NaN\n",
       "1945  7290613987  2019              NaN\n",
       "1946  7290613987  2020  martinshof.de/*\n",
       "\n",
       "[1947 rows x 3 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ee5ee-8841-427c-a909-8b2ccec9cf42",
   "metadata": {},
   "source": [
    "We now define some parameters for accessing CC and IA and also for storing the retrieved data in WARC files both via `cdx_toolkit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "august-photograph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = cdx_toolkit.CDXFetcher(source='ia')         # define client for fetching data from source (ia: Internet Archive, cc: Common Crawl)\n",
    "limit = 1000                                         # define maximum number of captures that is suppossed to be retrieved for each year-url from the respective archive\n",
    "crefos = list(df_url.crefo.drop_duplicates().values) # create list of unique company IDs (crefos) for which panel dataset of corporate website content is created\n",
    "len(crefos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "confirmed-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 'warcinfo' record describes the records that follow it, up through end of file, end of input, or until next 'warcinfo' record.\n",
    "# Typically, this appears once and at the beginning of a WARC file. \n",
    "# For a web archive, it often contains information about the web crawl which generated the following records.\n",
    "warcinfo = {\n",
    "    'software': 'pypi_cdx_toolkit iter-and-warc',\n",
    "    'isPartOf': 'GREENWASHING-SAMPLE-IA',\n",
    "    'description': 'warc extraction',\n",
    "    'format': 'WARC file version 1.0',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b0f95654-e2fd-4b80-9766-753b485523fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ) Crefo:  5050292256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) Crefo:  7010096344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [01:30,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ) Crefo:  3270014138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for year in range(2010, 2011):\n",
    "    writer = cdx_toolkit.warc.CDXToolkitWARCWriter(\n",
    "        prefix='example',         # first part of .warc file where warc records will be stored\n",
    "        subprefix=str(year),     # second part of .warc file where warc records will be stored\n",
    "        info=warcinfo,           \n",
    "        size=1000000000,         # once the .warc file exceeds 1 GB of size a new .warc file will be created for succeeding records\n",
    "        gzip=True)            \n",
    "    \n",
    "    for i, crefo in enumerate(crefos[0:3]):\n",
    "        row = df_url.loc[(df_url.crefo==crefo) & (df_url.year == year),:].squeeze(axis=0)\n",
    "        \n",
    "        if pd.isna(row.url):\n",
    "            pass                 # pass if firm has not existed in the respective (which refers to the url entry is missing for the respective year)\n",
    "        else:\n",
    "            print(str(i), '- Crefo: ', str(row.crefo))\n",
    "            for obj in tqdm(client.iter(row.url, from_ts=str(row.year), to=str(row.year), limit=limit, verbose='v', collapse='urlkey', filter=['status:200', 'mime:text/html'])):\n",
    "                url = obj['url']\n",
    "                status = obj['status']\n",
    "                timestamp = obj['timestamp']\n",
    "\n",
    "                try:\n",
    "                    record = obj.fetch_warc_record()\n",
    "                    # Save crefo into header information of the WARC record so it is not lost in the WARC file\n",
    "                    record.rec_headers['crefo'] = str(row.crefo)\n",
    "                except RuntimeError:\n",
    "                    print('Skipping capture for RuntimeError 404: %s %s', url, timestamp)\n",
    "                    continue\n",
    "                writer.write_record(record)\n",
    "                \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ea3060-fca9-4f49-9801-23f10a3ab84e",
   "metadata": {},
   "source": [
    "The above code snippet has created gzipped WARC file. The output shows the number of archived webpages for the respective crefo on the Internat Archive in the year 2010. The size of both the compressed and the uncompressed file is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e7bc807d-9f62-456e-af14-0a61dc187575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.055119 MB\n",
      "0.289178 MB\n"
     ]
    }
   ],
   "source": [
    "get_uncompressed_size(r'Q:\\Meine Bibliotheken\\Research\\05_Code\\02_ArchiveSpark\\example-2010-000000.extracted.warc.gz')\n",
    "get_compressed_size(r'Q:\\Meine Bibliotheken\\Research\\05_Code\\02_ArchiveSpark\\example-2010-000000.extracted.warc.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585e24d0-6569-4a6c-b87a-d7da3568aab6",
   "metadata": {},
   "source": [
    "We see that the file size reduces by factor 5 when using GZIP for compressing the WARC file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9963378-1250-4ec0-9cce-31b9503cc275",
   "metadata": {},
   "source": [
    "## Read .warc files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f658b98-df7d-4690-bc93-c568f6a146cf",
   "metadata": {},
   "source": [
    "Again, the stored WARC file and its headers and payloads can be easily accessed via warcio's `ArchiveIterator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b28a5f23-5fa8-41c5-a37c-68ac85ebaae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warcio.archiveiterator import ArchiveIterator\n",
    "import pandas as pd # used to store results in a data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd4381-56d2-4804-819a-4af1fee82c56",
   "metadata": {},
   "source": [
    "Header information (metadate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e97c0371-87fa-4978-9051-967b5834d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_header = pd.DataFrame()\n",
    "with open(r'Q:\\Meine Bibliotheken\\Research\\05_Code\\02_ArchiveSpark\\example-2010-000000.extracted.warc.gz', 'rb') as stream:\n",
    "    for record in ArchiveIterator(stream):\n",
    "        if record.rec_headers['WARC-Type'] == 'warcinfo':\n",
    "            pass\n",
    "        else:\n",
    "            temp = sorted([(i[0], [i[1]]) for i in record.rec_headers.headers if i[0] in ['crefo', 'WARC-Date', 'Content-Type', 'Content-Length', 'WARC-Source-URI']], key=lambda tup: tup[0], reverse=True) \n",
    "            df_temp = pd.DataFrame.from_dict(dict(temp))\n",
    "            df_header = df_header.append(df_temp)\n",
    "stream.close()\n",
    "\n",
    "df_header.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "9de9d11c-5a83-4719-9d49-0852a18b83fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crefo</th>\n",
       "      <th>WARC-Source-URI</th>\n",
       "      <th>WARC-Date</th>\n",
       "      <th>Content-Type</th>\n",
       "      <th>Content-Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5050292256</td>\n",
       "      <td>https://web.archive.org/web/20100402084249id_/http%3A//www.tulip.de%3A80/</td>\n",
       "      <td>2010-04-02T08:42:52Z</td>\n",
       "      <td>application/http; msgtype=response</td>\n",
       "      <td>36018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7010096344</td>\n",
       "      <td>https://web.archive.org/web/20100327143928id_/http%3A//www.omya.de%3A80/</td>\n",
       "      <td>2010-03-27T14:56:08Z</td>\n",
       "      <td>application/http; msgtype=response</td>\n",
       "      <td>10134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7010096344</td>\n",
       "      <td>https://web.archive.org/web/20100824072815id_/http%3A//www.omya.de%3A80/C12574C800506F23/direct/Home</td>\n",
       "      <td>2010-08-24T07:48:58Z</td>\n",
       "      <td>application/http; msgtype=response</td>\n",
       "      <td>16723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7010096344</td>\n",
       "      <td>https://web.archive.org/web/20100824072820id_/http%3A//www.omya.de%3A80/C12574C800506F23/vwWebPagesByID/1AA35D5E7DF8930AC12574E300474F38</td>\n",
       "      <td>2010-08-24T07:49:04Z</td>\n",
       "      <td>application/http; msgtype=response</td>\n",
       "      <td>18842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7010096344</td>\n",
       "      <td>https://web.archive.org/web/20100802002229id_/http%3A//www.omya.de%3A80/C12574C800506F23/vwWebPagesByID/1C593814E6E896D2C12574E300474FEC</td>\n",
       "      <td>2010-08-02T00:42:36Z</td>\n",
       "      <td>application/http; msgtype=response</td>\n",
       "      <td>16002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        crefo  \\\n",
       "0  5050292256   \n",
       "1  7010096344   \n",
       "2  7010096344   \n",
       "3  7010096344   \n",
       "4  7010096344   \n",
       "\n",
       "                                                                                                                            WARC-Source-URI  \\\n",
       "0                                                                 https://web.archive.org/web/20100402084249id_/http%3A//www.tulip.de%3A80/   \n",
       "1                                                                  https://web.archive.org/web/20100327143928id_/http%3A//www.omya.de%3A80/   \n",
       "2                                      https://web.archive.org/web/20100824072815id_/http%3A//www.omya.de%3A80/C12574C800506F23/direct/Home   \n",
       "3  https://web.archive.org/web/20100824072820id_/http%3A//www.omya.de%3A80/C12574C800506F23/vwWebPagesByID/1AA35D5E7DF8930AC12574E300474F38   \n",
       "4  https://web.archive.org/web/20100802002229id_/http%3A//www.omya.de%3A80/C12574C800506F23/vwWebPagesByID/1C593814E6E896D2C12574E300474FEC   \n",
       "\n",
       "              WARC-Date                        Content-Type Content-Length  \n",
       "0  2010-04-02T08:42:52Z  application/http; msgtype=response          36018  \n",
       "1  2010-03-27T14:56:08Z  application/http; msgtype=response          10134  \n",
       "2  2010-08-24T07:48:58Z  application/http; msgtype=response          16723  \n",
       "3  2010-08-24T07:49:04Z  application/http; msgtype=response          18842  \n",
       "4  2010-08-02T00:42:36Z  application/http; msgtype=response          16002  "
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_header.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12d3a5-24ce-4d0f-8d7c-357488fd4e90",
   "metadata": {},
   "source": [
    "And of course the content block (payload):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "96f4b28d-f083-43ff-9ae6-2146550ae0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "httpsweb.archive.org/web/20100402084249id_/http%3A//www.tulip.de%3A80/DE\n",
      "TULIP DeutschlandWillkommen auf der Homepage von Tulip, Germany.Bitte klicken Sie aufeinen der Buttons,um auf diejeweiligeHomepage zu gelangen:Tulip Food Company, DüsseldorfTulip Food Service, KielTulip Food Company, DüsseldorfAls Vertriebs-und Marketing-organisation mit Sitz in Düsseldorf beliefert die Tulip Food CompanyGmbH denEinzelhandel mit Kühl-und Tiefkühlprodukten sowieKonserven.Tulip Food Service, Kielbietet ein umfassendes GV-Sortiment tiefgekühlterundgekühlter Fleisch-Convenience-Produkte und ist attraktiverLieferant für dieweiterverarbeitendeIndustrie.\n",
      "\n",
      "\n",
      "httpsweb.archive.org/web/20100327143928id_/http%3A//www.omya.de%3A80/DE\n",
      "OMYA AG - white minerals, calcium carbonat and talcsOMYA Omya,paper,paints,plastics,adhesives,coatings,calcium carbonate,calcium,talc,white minerals,industrial minerals,fillers,extenders,pigments,chalk,limestone,marble,pcc,gcc,coating pigments Omya,Papier,Farben,Lacke,Kunststoff,Elastomere,Klebstoffe,Adhesive,Calciumcarbonat,Kalziumcarbonat,Calcium,Kalzium,Talk,Füllstoffe,Extender,Streichpigmente,Kreide,Kalkstein,Marmor,PCC,GCC,IndustriemineraleOmya is an international white minerals company supplying high quality calcium carbonate and talc. Omya zählt zu den weltweit führenden Hersteller von Industriemineralen aus Calciumcarbonat und Talkum.If your browser does not support automatic redirection clickhere\n",
      "\n",
      "\n",
      "httpsweb.archive.org/web/20100824072815id_/http%3A//www.omya.de%3A80/C12574C800506F23/direct/HomeDE\n",
      "- UnternehmenGo To Content (Skip Navigation)Main NavigationUnternehmenMärkteProdukteKompetenzenKontaktKarriereSearch FormSubnavigation \"Unternehmen\"GeschichteStandorteNachhaltigkeitQualitätREAChOmya weltweitBreadcrumb TrailUnternehmenendeWillkommen bei Omya in DeutschlandDie Omya GmbH ist der führende Anbieter von Industriemineralen auf der Basis von Calciumcarbonat und Dolomit sowie deutschlandweit in der Distribution von Spezialchemikalien tätig.Mit Verkaufsbüros und Außendienstmitarbeitern in allen Regionen Deutschlands sind wir nah an den Märkten und dicht bei den Kunden. So können wir schnell auf Anfragen reagieren, dementsprechend kurz sind unsere Lieferzeiten.Unsere Werke liegen dort, wo die hochwertigsten Kreide- und Kalkstein-Vorkommen in Deutschland zu finden sind: Auf der Schwäbischen Alb und entlang des norddeutschen Kreidegürtels. Darüber hinaus greifen wir auf das breite Produktangebot von mehr als 50 Omya-Werken in Europa zurück.Höchste QualitätDer Einsatz modernster Technik ist der Schlüssel, um aus dem Naturprodukt Calciumcarbonat maßgeschneiderte Produkte für die unterschiedlichsten Anwendungen herstellen zu können.Latest HeadlinesGo To Main NavigationGo To ServicenavigationCopyright Omya AG 2010|Sitemap|Kontakt|Privacy Policy & Terms of Use\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(r'Q:\\Meine Bibliotheken\\Research\\05_Code\\02_ArchiveSpark\\example-2010-000000.extracted.warc.gz', 'rb') as stream:\n",
    "    for i, record in enumerate(ArchiveIterator(stream)):\n",
    "        if record.http_headers is None:\n",
    "            pass\n",
    "        else:\n",
    "            print(record.http_headers['X-Archive-X-Cache-Key'] + '\\n' + BeautifulSoup(record.content_stream().read(), 'html.parser').get_text(strip=True))\n",
    "            print('\\n')\n",
    "            if i > 2:\n",
    "                break\n",
    "stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020fa40-e465-4127-9047-6b2ed4716638",
   "metadata": {},
   "source": [
    "# Why WARC files?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0aa2b-4f8f-4748-9560-3070981e2a98",
   "metadata": {},
   "source": [
    "Besides WARC being the ISO standard for storing webdata, working with WARC data comes with another big advantage. It allows to process the webdata collections with cluster computing frameworks such as Hadoop and Spark. These frameworks allow efficient data processing, extraction as well as derivation for large archival collections (i.e. WARC files). One specific framework in this context is [ArchiveSpark](https://github.com/helgeho/ArchiveSpark). The main use case of ArchiveSpark is the efficient access to archival data with the goal to derive corpora by applying filters and tools in order to extract information from the original raw data, to be stored in a more accessible format, like JSON, while reflecting the data lineage of each derived value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe870c-8a2d-474d-8587-fb6bc4fab88a",
   "metadata": {},
   "source": [
    "<img src=\"https://www.clipartmax.com/png/small/78-780281_whats-new-in-apache-spark-apache-spark-logo.png\" alt=\"What's New In Apache Spark - Apache Spark Logo @clipartmax.com\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1733afad-25e5-4cbd-9e4b-f63f654d6d48",
   "metadata": {},
   "source": [
    "So even if we have reduced the large web archives of CC and IA to a collection of only corporate website data, we will still be talking about data volume close to 1 TB (my personal estimation). To derive from these collection of corporate website data smaller corpora which only comprise web content that is relevant for the specific research question at hand, ArchiveSpark and similar cluster computing solutions are the way to go."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
